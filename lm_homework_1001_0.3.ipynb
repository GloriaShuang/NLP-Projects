{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "lm_homework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GloriaShuang/NLP-Projects/blob/master/lm_homework_1001_0.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls2hblX5r6ad",
        "colab_type": "text"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR5LTSGnswNV",
        "colab_type": "code",
        "outputId": "7c388e85-2c2b-4d8c-e073-7ebc3dfb8fad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCD0C2fz7bxa",
        "colab_type": "code",
        "outputId": "8a44b986-12e9-4733-a72c-a86de0fb136d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKuN-wmWr6af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN_NhePJr6aj",
        "colab_type": "text"
      },
      "source": [
        "## I. N-Gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgoJr6Ldr6ak",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMaf6xvXr6al",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdPlIYf_r6ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramAdditive(object):\n",
        "    def __init__(self, n, delta, vsize):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "            for i in range(len(padded_sequence) - self.n+1):\n",
        "                ngram = tuple(padded_sequence[i:i+self.n])\n",
        "                prefix, word = ngram[:-1], ngram[-1]\n",
        "                self.count[prefix][word] += 1\n",
        "                self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        prob = ((self.delta + self.count[prefix][word]) / \n",
        "                (self.total[prefix] + self.delta*self.vsize))\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-G1XSMLr6av",
        "colab_type": "code",
        "outputId": "bd09c8a8-cae8-499d-dfa2-dfaf851146d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        }
      },
      "source": [
        "datasets, vocab = load_wikitext()\n",
        "\n",
        "delta = 0.0005\n",
        "for n in [2, 3, 4]:\n",
        "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-01 21:22:37--  https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.box.com (nyu.box.com)... 103.116.4.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|103.116.4.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-10-01 21:22:37--  https://nyu.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-10-01 21:22:38--  https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 103.116.4.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|103.116.4.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!MKsVbJWOzz5Tjq0ftlzSY6_N2S9R9grnqH3ASlHYt5FMTXExCrEj6NacZpkVgZHEF02Civ-g9gBLbQRH5MNiossfQDI6-G_Fay7YwlhzxWA0eLlUlspRbEttpMlVG9W8fx7jAgW7sR8VdMm_VjZtLdpKQWvFGAHUrOClQ5RKXtaxlrAZS3mjhWEpk1uTDMIjIIANx8rANZjMLNiWIBGldmErg1YY-587LaYzH2BRstQ7-ZfBOqvaNyTJJWucA03moE6J4iseTdhtE9NLQnG3LbrnqyhXrtM-dBo1sf2MvU4YPgG32Y3NW3dkVnJbCnGLWz8VwBGrUCWMUhgLVf_WJKYgZ5WiTl4LW0GwZdMUuXQ2uB5CFtszgT6KFo5pqK3X9L5CEGPymoYPazVhMeBNywK-jA9lLYdIWiRLZ0cqWUo6dMpKXeE4s-sSuAHM1CGkoS3Zt_1kHXGhflYsVU0o5HZQDlLHp3D_4S645TrxYfIudli7z3D6o3xK1mHVQPKST7npc3ALbmEK9eBwKs_Sn4XR7RSgyr5VnwH1nd4-_Qd3gRVqa_9ZNPU2l7xdiS2HOyBF6R0Kg5dArcUtWdIr_8hAkBn12auWOFG6iUHdPa7l2y0jYJumtxV_tB50JAo3dHs40Nzcfj7fpYUmmuQRo-AEgt8btZoFX1SsEWfnAblkkRR6ZCT4Z-LLMCEgkhDNwAzcWx_M3KhKG_NirkxeZxoPeoP3xmTEQvBHKyZwj3xFpM0i8LDWpxITpqHVfVAkSH3AQsSnhTQ8zhiVvuqlAbDlkvKODMtgGbFggahwgMtBZgWx1z78-tmC6r_krQuoqzCVgNxG4yft63-3Kk5NCb9x95XHJGEHQgS7eNsw5JPvE5z8ORd9tfQBlsQM8i1Fjp0aZr9BepHrSTZ9HutoIPE84hGiz4sjxVBRUfZKKcait3-wTFCb3PL6sjQW9-NSpY94bIHxraCvZ1Cj5HfwPbfXwjQtIMzB5EVkEn2jtZIqaQ2kM-Uo2CkNHGAQtjfvcBjW8doWedou5JfM8-ky93P79J1wCCz7ego_Lh87rixRTe5E2C34Kbmikn5ybdqWtmH-RVUDihFg5pzx5_rrKmuEFFcwTCIKw6eLiyMXjpKZi0iSsB3m4PQEgpGXScSr6gC2gj9zWUOVTFHvzKPyrBitMH-HNI4a9EAgOIeBtvzgjxkMxSS92HOSujuI_H3SXBSt1uK8iCQ7hgprMY5VvFi-CyknHSLLvaJZo-AJ-ErbvuOWpSb9Y9VKEe_wtXTKkaeXjCwkLD0Nw6N589y10lyVE7ZecMQPaxEaGEH_NvyWyHszR0gfIDoIeyXV8EIZlDeDD_uLU84AgEL1CpiGCTe_glTUcwd4QS3O9KX31lbHm_OWEk8BnQKmbethI-D8eJSRromLElz68wc7/download [following]\n",
            "--2019-10-01 21:22:38--  https://public.boxcloud.com/d/1/b1!MKsVbJWOzz5Tjq0ftlzSY6_N2S9R9grnqH3ASlHYt5FMTXExCrEj6NacZpkVgZHEF02Civ-g9gBLbQRH5MNiossfQDI6-G_Fay7YwlhzxWA0eLlUlspRbEttpMlVG9W8fx7jAgW7sR8VdMm_VjZtLdpKQWvFGAHUrOClQ5RKXtaxlrAZS3mjhWEpk1uTDMIjIIANx8rANZjMLNiWIBGldmErg1YY-587LaYzH2BRstQ7-ZfBOqvaNyTJJWucA03moE6J4iseTdhtE9NLQnG3LbrnqyhXrtM-dBo1sf2MvU4YPgG32Y3NW3dkVnJbCnGLWz8VwBGrUCWMUhgLVf_WJKYgZ5WiTl4LW0GwZdMUuXQ2uB5CFtszgT6KFo5pqK3X9L5CEGPymoYPazVhMeBNywK-jA9lLYdIWiRLZ0cqWUo6dMpKXeE4s-sSuAHM1CGkoS3Zt_1kHXGhflYsVU0o5HZQDlLHp3D_4S645TrxYfIudli7z3D6o3xK1mHVQPKST7npc3ALbmEK9eBwKs_Sn4XR7RSgyr5VnwH1nd4-_Qd3gRVqa_9ZNPU2l7xdiS2HOyBF6R0Kg5dArcUtWdIr_8hAkBn12auWOFG6iUHdPa7l2y0jYJumtxV_tB50JAo3dHs40Nzcfj7fpYUmmuQRo-AEgt8btZoFX1SsEWfnAblkkRR6ZCT4Z-LLMCEgkhDNwAzcWx_M3KhKG_NirkxeZxoPeoP3xmTEQvBHKyZwj3xFpM0i8LDWpxITpqHVfVAkSH3AQsSnhTQ8zhiVvuqlAbDlkvKODMtgGbFggahwgMtBZgWx1z78-tmC6r_krQuoqzCVgNxG4yft63-3Kk5NCb9x95XHJGEHQgS7eNsw5JPvE5z8ORd9tfQBlsQM8i1Fjp0aZr9BepHrSTZ9HutoIPE84hGiz4sjxVBRUfZKKcait3-wTFCb3PL6sjQW9-NSpY94bIHxraCvZ1Cj5HfwPbfXwjQtIMzB5EVkEn2jtZIqaQ2kM-Uo2CkNHGAQtjfvcBjW8doWedou5JfM8-ky93P79J1wCCz7ego_Lh87rixRTe5E2C34Kbmikn5ybdqWtmH-RVUDihFg5pzx5_rrKmuEFFcwTCIKw6eLiyMXjpKZi0iSsB3m4PQEgpGXScSr6gC2gj9zWUOVTFHvzKPyrBitMH-HNI4a9EAgOIeBtvzgjxkMxSS92HOSujuI_H3SXBSt1uK8iCQ7hgprMY5VvFi-CyknHSLLvaJZo-AJ-ErbvuOWpSb9Y9VKEe_wtXTKkaeXjCwkLD0Nw6N589y10lyVE7ZecMQPaxEaGEH_NvyWyHszR0gfIDoIeyXV8EIZlDeDD_uLU84AgEL1CpiGCTe_glTUcwd4QS3O9KX31lbHm_OWEk8BnQKmbethI-D8eJSRromLElz68wc7/download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 103.116.4.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|103.116.4.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12714601 (12M) [application/octet-stream]\n",
            "Saving to: ‘wikitext2-sentencized.json’\n",
            "\n",
            "wikitext2-sentenciz 100%[===================>]  12.12M  9.13MB/s    in 1.3s    \n",
            "\n",
            "2019-10-01 21:22:41 (9.13 MB/s) - ‘wikitext2-sentencized.json’ saved [12714601/12714601]\n",
            "\n",
            "Vocab size: 33175\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Train Perplexity: 90.228\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Valid Perplexity: 525.825\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Train Perplexity: 26.768\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Valid Perplexity: 2577.128\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Train Perplexity: 19.947\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Valid Perplexity: 9570.901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwYcm_4_r6a5",
        "colab_type": "text"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6reLnvo8r6a6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWVI0CcSr6a9",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvPDBRIlanKB",
        "colab_type": "text"
      },
      "source": [
        "##### Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3EnUteHr6a_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "try:\n",
        "    import jsonlines\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install jsonlines\n",
        "  \n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install tqdm\n",
        "  \n",
        "import os\n",
        "    \n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        \n",
        "        # add special tokens\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>')\n",
        "        \n",
        "        for line in tqdm(datasets['train']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah0lzi7tawdD",
        "colab_type": "text"
      },
      "source": [
        " ##### Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDmdwmMIa2DC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IFmtY8ka516",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function slices the input sequence into ngrams i.e.\n",
        "# [0,1,2,3,4,5] will be sliced into bigrams\n",
        "# [0,1], [1,2], [2,3], [3,4], [4,5] etc for bigger orders\n",
        "\n",
        "def slice_sequences_given_order(tokenized_dataset_with_spec, ngram_order=5):\n",
        "    sliced_datasets = {}\n",
        "    for split, dataset in tokenized_dataset_with_spec.items():\n",
        "        _list_of_sliced_ngrams = []\n",
        "        for seq in tqdm(dataset):\n",
        "            ngrams = [seq[i:i+ngram_order] for i in range(len(seq)-ngram_order+1)]\n",
        "            _list_of_sliced_ngrams.extend(ngrams)\n",
        "        sliced_datasets[split] = _list_of_sliced_ngrams\n",
        "\n",
        "    return sliced_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0HKbWcRco-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "  \n",
        "  \n",
        "# wiki_dataset, vocab = load_wikitext()\n",
        "\n",
        "# print(wiki_dataset['train'][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4CyOro6bu7l",
        "colab_type": "code",
        "outputId": "44680787-a4fa-4991-9bfc-2be65b615ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "\n",
        "wiki_dataset, wiki_dict_ori = load_wikitext()\n",
        "\n",
        "wiki_dict = Dictionary(wiki_dataset, include_valid=True)\n",
        "\n",
        "# checking some example\n",
        "print(' '.join(wiki_dataset['train'][3010]))\n",
        "\n",
        "encoded = wiki_dict.encode_token_seq(wiki_dataset['train'][3010])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = wiki_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')\n",
        "\n",
        "# personachat_tokenized_datasets_5gram = tokenize_dataset(wiki_dataset, wiki_dict, ngram_order=5)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 458/78274 [00:00<00:17, 4576.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 33175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [02:17<00:00, 570.25it/s]\n",
            "100%|██████████| 8464/8464 [00:10<00:00, 779.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Nataraja and Ardhanarishvara sculptures are also attributed to the Rashtrakutas .\n",
            "\n",
            " encoded - [75, 8816, 30, 8817, 8732, 70, 91, 2960, 13, 6, 8806, 39]\n",
            "\n",
            " decoded - ['The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gxZ_zTsghrR",
        "colab_type": "code",
        "outputId": "100b386f-3e9c-4943-d1e2-be086c8b37a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "print(' '.join(wiki_dataset['train'][3010]))\n",
        "encoded = wiki_dict.encode_token_seq(wiki_dataset['train'][3010])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = wiki_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')\n",
        "\n",
        "print(len(wiki_dict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Nataraja and Ardhanarishvara sculptures are also attributed to the Rashtrakutas .\n",
            "\n",
            " encoded - [75, 8816, 30, 8817, 8732, 70, 91, 2960, 13, 6, 8806, 39]\n",
            "\n",
            " decoded - ['The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.']\n",
            "33181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glve_RgAr6bB",
        "colab_type": "text"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5l_aw6Ty4L8",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Define the dataset and dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4KvilAdzPPp",
        "colab_type": "text"
      },
      "source": [
        "##### 1.1 Define the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqejGY-xisZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8qIsQu9iw5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    #pad_token = persona_dict.get_id('<pad>')\n",
        "    pad_token = 2\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFpEtkvfzW_c",
        "colab_type": "text"
      },
      "source": [
        "##### 1.2 define the tokenized datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L28tfUVi994",
        "colab_type": "code",
        "outputId": "c8fc5f97-2237-48cf-c94c-7455a67049d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "print(wiki_dataset['train'][1])\n",
        "\n",
        "encoded = wiki_dict.encode_token_seq(wiki_dataset['train'][1])\n",
        "print(wiki_dict.decode_idx_seq(encoded))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Released', 'in', 'January', '2011', 'in', 'Japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'Valkyria', 'series', '.']\n",
            "['Released', 'in', 'January', '2011', 'in', 'Japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'Valkyria', 'series', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-LuHCBji06I",
        "colab_type": "code",
        "outputId": "82fe7cbc-9e7c-4c72-d0d4-8bab5adcd5aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "wiki_tokenized_datasets = tokenize_dataset(wiki_dataset, wiki_dict)\n",
        "wiki_tensor_dataset = {}\n",
        "\n",
        "for split, listoflists in wiki_tokenized_datasets.items():\n",
        "    wiki_tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "# check the first example\n",
        "wiki_tensor_dataset['train'][13]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:00<00:00, 120058.08it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 125508.97it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 128737.65it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  0, 171,  70, 172, 173, 174, 175,  23, 176, 177, 111, 178, 179, 180,\n",
              "           10, 111, 181, 182, 183, 173, 184, 185, 186,  30, 183, 173,   3, 187,\n",
              "           39]]),\n",
              " tensor([[171,  70, 172, 173, 174, 175,  23, 176, 177, 111, 178, 179, 180,  10,\n",
              "          111, 181, 182, 183, 173, 184, 185, 186,  30, 183, 173,   3, 187,  39,\n",
              "            1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRs0_lwruelA",
        "colab_type": "text"
      },
      "source": [
        "##### Define the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLX0frRpjtz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_loaders = {}\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
        "    wiki_loaders[split] = DataLoader(wiki_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYg2IUPbzs5C",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Define the model and initialize the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGvIpR_surrl",
        "colab_type": "text"
      },
      "source": [
        "##### 1. define the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-8GgNQKz2Zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.LSTM = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        rnn_outputs = self.LSTM(embeddings)\n",
        "        logits = self.projection(rnn_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcJTRulBpUVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "\n",
        "if not os.path.exists('/content/drive/My Drive/nlp2_save_model'):\n",
        "  os.mkdir('/content/drive/My Drive/nlp2_save_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18xcFVsV4aAP",
        "colab_type": "code",
        "outputId": "7d70af15-1335-4dd2-e9ee-c1c5bdcec2b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# creating a LSTM model, criterion and optimizer\n",
        "\n",
        "\n",
        "# create the model folder\n",
        "epoch_num = 0\n",
        "load_pretrained = False\n",
        "\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "if load_pretrained:\n",
        "    # input the epoch num you want to continue based on\n",
        "    epoch_num = 0\n",
        "    \n",
        "    embedding_size = 256\n",
        "    hidden_size = 300\n",
        "    num_layers = 1\n",
        "    rnn_dropout = 0.3\n",
        "\n",
        "    model_predix = model_predix = '1001_LSTM_'+'dim_'+str(embedding_size)+'hidden_size_'+str(hidden_size)+'rnn_dropout_'+str(rnn_dropout)+ 'num_layers_' +str(num_layers)\n",
        "    path_root = '/content/drive/My Drive/nlp2_save_model/' + model_predix\n",
        "\n",
        "    option_path = os.path.join(path_root,'options.pickle')\n",
        "    f = open(option_path, 'rb')\n",
        "    options = pickle.load(f)\n",
        "\n",
        "  \n",
        "    model_name = 'lstm' + str(epoch_num)+'.pth'\n",
        "    model_path = os.path.join(path_root,model_name)\n",
        "    if not os.path.exists(model_path):\n",
        "        raise EOFError('Download pretrained model!')\n",
        "    model_dict = torch.load(model_path)\n",
        "    model = LSTMLanguageModel(options).to(current_device)\n",
        "    model.load_state_dict(model_dict)\n",
        "    \n",
        "else:\n",
        "  # setting the new model parametter\n",
        "#     embedding_size = 256\n",
        "#     hidden_size = 512\n",
        "#     num_layers = 3\n",
        "#     rnn_dropout = 0.3\n",
        "    embedding_size = 256\n",
        "    hidden_size = 300\n",
        "    num_layers = 1\n",
        "    rnn_dropout = 0.3\n",
        "    \n",
        "    model_predix = model_predix = '1001_LSTM_'+'dim_'+str(embedding_size)+'hidden_size_'+str(hidden_size)+'rnn_dropout_'+str(rnn_dropout)+ 'num_layers_' + str(num_layers)\n",
        "    path_root = '/content/drive/My Drive/nlp2_save_model/' + model_predix\n",
        "\n",
        "    if not os.path.exists(path_root):\n",
        "      os.mkdir(path_root)\n",
        "    \n",
        "    options = {\n",
        "        'num_embeddings': len(wiki_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': wiki_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'rnn_dropout': rnn_dropout,\n",
        "    }\n",
        "    \n",
        "    model_para = options\n",
        "    \n",
        "    with open(os.path.join(path_root,'options.pickle'), 'wb') as handle:\n",
        "      pickle.dump(model_para, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    model = LSTMLanguageModel(options).to(current_device)\n",
        "\n",
        "    \n",
        "    \n",
        "criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'))\n",
        "\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(model_parameters, lr=0.001, momentum=0.999)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5YLhj3-4ME0",
        "colab_type": "code",
        "outputId": "91e0a805-e9f2-4394-daad-0d6ee3574917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMLanguageModel(\n",
              "  (lookup): Embedding(33181, 256, padding_idx=2)\n",
              "  (LSTM): LSTM(256, 300, batch_first=True, dropout=0.3)\n",
              "  (projection): Linear(in_features=300, out_features=33181, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC6dqZhbkGi4",
        "colab_type": "code",
        "outputId": "facc262b-1ffc-4401-e0e2-7a9bae9a434e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# now we make same training loop, now with dataset and the model\n",
        "import torch \n",
        "import pickle\n",
        "\n",
        "model_para = options\n",
        "\n",
        "\n",
        "# define the epoch number of start training\n",
        "\n",
        "if load_pretrained == True:\n",
        "  epoch_now = epoch_num +1\n",
        "else:\n",
        "  epoch_now = 0\n",
        "  \n",
        "# if using pretrained model, the load the previous plot_cache \n",
        "if load_pretrained:\n",
        "  plot_cache = torch.load(os.path.join(path_root,'plot_acc'))\n",
        "else:\n",
        "  plot_cache = []\n",
        "    \n",
        "    \n",
        "# start training\n",
        "for epoch_number in range(epoch_now,100):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        # do train\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(wiki_loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "                \n",
        "    model_path = os.path.join(path_root,'lstm'+ str(epoch_number)+\".pth\")            \n",
        "    torch.save(model.state_dict(), model_path)           \n",
        "                \n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(wiki_loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "#     if load_pretrained:\n",
        "#         break\n",
        "        \n",
        "torch.save(plot_cache, os.path.join(path_root,'plot_acc'))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 10.4201\n",
            "Step 100 avg train loss = 10.3716\n",
            "Step 200 avg train loss = 10.0798\n",
            "Step 300 avg train loss = 9.3163\n",
            "Step 400 avg train loss = 8.5175\n",
            "Step 500 avg train loss = 8.2100\n",
            "Step 600 avg train loss = 7.9637\n",
            "Step 700 avg train loss = 7.7632\n",
            "Step 800 avg train loss = 7.7155\n",
            "Step 900 avg train loss = 7.5711\n",
            "Step 1000 avg train loss = 7.4836\n",
            "Step 1100 avg train loss = 7.4365\n",
            "Step 1200 avg train loss = 7.3373\n",
            "Validation loss after 0 epoch = 7.1523\n",
            "Step 0 avg train loss = 7.2859\n",
            "Step 100 avg train loss = 7.2802\n",
            "Step 200 avg train loss = 7.2160\n",
            "Step 300 avg train loss = 7.1749\n",
            "Step 400 avg train loss = 7.1063\n",
            "Step 500 avg train loss = 7.0531\n",
            "Step 600 avg train loss = 7.0193\n",
            "Step 700 avg train loss = 6.9520\n",
            "Step 800 avg train loss = 6.9078\n",
            "Step 900 avg train loss = 6.8756\n",
            "Step 1000 avg train loss = 6.8456\n",
            "Step 1100 avg train loss = 6.8129\n",
            "Step 1200 avg train loss = 6.7719\n",
            "Validation loss after 1 epoch = 6.5664\n",
            "Step 0 avg train loss = 6.7693\n",
            "Step 100 avg train loss = 6.7109\n",
            "Step 200 avg train loss = 6.6709\n",
            "Step 300 avg train loss = 6.6453\n",
            "Step 400 avg train loss = 6.6016\n",
            "Step 500 avg train loss = 6.5755\n",
            "Step 600 avg train loss = 6.5520\n",
            "Step 700 avg train loss = 6.5118\n",
            "Step 800 avg train loss = 6.4873\n",
            "Step 900 avg train loss = 6.4431\n",
            "Step 1000 avg train loss = 6.4368\n",
            "Step 1100 avg train loss = 6.4057\n",
            "Step 1200 avg train loss = 6.3927\n",
            "Validation loss after 2 epoch = 6.2029\n",
            "Step 0 avg train loss = 6.4372\n",
            "Step 100 avg train loss = 6.3330\n",
            "Step 200 avg train loss = 6.3285\n",
            "Step 300 avg train loss = 6.3107\n",
            "Step 400 avg train loss = 6.2763\n",
            "Step 500 avg train loss = 6.2613\n",
            "Step 600 avg train loss = 6.2429\n",
            "Step 700 avg train loss = 6.2133\n",
            "Step 800 avg train loss = 6.2058\n",
            "Step 900 avg train loss = 6.1870\n",
            "Step 1000 avg train loss = 6.1794\n",
            "Step 1100 avg train loss = 6.1508\n",
            "Step 1200 avg train loss = 6.1428\n",
            "Validation loss after 3 epoch = 5.9831\n",
            "Step 0 avg train loss = 6.1305\n",
            "Step 100 avg train loss = 6.0991\n",
            "Step 200 avg train loss = 6.0919\n",
            "Step 300 avg train loss = 6.0717\n",
            "Step 400 avg train loss = 6.0726\n",
            "Step 500 avg train loss = 6.0593\n",
            "Step 600 avg train loss = 6.0519\n",
            "Step 700 avg train loss = 6.0351\n",
            "Step 800 avg train loss = 6.0353\n",
            "Step 900 avg train loss = 6.0153\n",
            "Step 1000 avg train loss = 6.0036\n",
            "Step 1100 avg train loss = 5.9859\n",
            "Step 1200 avg train loss = 5.9670\n",
            "Validation loss after 4 epoch = 5.8474\n",
            "Step 0 avg train loss = 5.8401\n",
            "Step 100 avg train loss = 5.9605\n",
            "Step 200 avg train loss = 5.9371\n",
            "Step 300 avg train loss = 5.9329\n",
            "Step 400 avg train loss = 5.9191\n",
            "Step 500 avg train loss = 5.9251\n",
            "Step 600 avg train loss = 5.9176\n",
            "Step 700 avg train loss = 5.8964\n",
            "Step 800 avg train loss = 5.8906\n",
            "Step 900 avg train loss = 5.8733\n",
            "Step 1000 avg train loss = 5.8660\n",
            "Step 1100 avg train loss = 5.8735\n",
            "Step 1200 avg train loss = 5.8524\n",
            "Validation loss after 5 epoch = 5.7536\n",
            "Step 0 avg train loss = 6.0372\n",
            "Step 100 avg train loss = 5.8318\n",
            "Step 200 avg train loss = 5.8291\n",
            "Step 300 avg train loss = 5.8192\n",
            "Step 400 avg train loss = 5.8017\n",
            "Step 500 avg train loss = 5.7992\n",
            "Step 600 avg train loss = 5.8033\n",
            "Step 700 avg train loss = 5.7891\n",
            "Step 800 avg train loss = 5.7872\n",
            "Step 900 avg train loss = 5.7853\n",
            "Step 1000 avg train loss = 5.7708\n",
            "Step 1100 avg train loss = 5.7577\n",
            "Step 1200 avg train loss = 5.7749\n",
            "Validation loss after 6 epoch = 5.6809\n",
            "Step 0 avg train loss = 5.7756\n",
            "Step 100 avg train loss = 5.7432\n",
            "Step 200 avg train loss = 5.7297\n",
            "Step 300 avg train loss = 5.7213\n",
            "Step 400 avg train loss = 5.7167\n",
            "Step 500 avg train loss = 5.7170\n",
            "Step 600 avg train loss = 5.7028\n",
            "Step 700 avg train loss = 5.7042\n",
            "Step 800 avg train loss = 5.6857\n",
            "Step 900 avg train loss = 5.7015\n",
            "Step 1000 avg train loss = 5.6837\n",
            "Step 1100 avg train loss = 5.6880\n",
            "Step 1200 avg train loss = 5.6913\n",
            "Validation loss after 7 epoch = 5.6249\n",
            "Step 0 avg train loss = 5.5380\n",
            "Step 100 avg train loss = 5.6559\n",
            "Step 200 avg train loss = 5.6438\n",
            "Step 300 avg train loss = 5.6515\n",
            "Step 400 avg train loss = 5.6408\n",
            "Step 500 avg train loss = 5.6406\n",
            "Step 600 avg train loss = 5.6371\n",
            "Step 700 avg train loss = 5.6432\n",
            "Step 800 avg train loss = 5.6091\n",
            "Step 900 avg train loss = 5.6320\n",
            "Step 1000 avg train loss = 5.6139\n",
            "Step 1100 avg train loss = 5.6052\n",
            "Step 1200 avg train loss = 5.6062\n",
            "Validation loss after 8 epoch = 5.5794\n",
            "Step 0 avg train loss = 5.4058\n",
            "Step 100 avg train loss = 5.5858\n",
            "Step 200 avg train loss = 5.5917\n",
            "Step 300 avg train loss = 5.5600\n",
            "Step 400 avg train loss = 5.5734\n",
            "Step 500 avg train loss = 5.5736\n",
            "Step 600 avg train loss = 5.5514\n",
            "Step 700 avg train loss = 5.5617\n",
            "Step 800 avg train loss = 5.5548\n",
            "Step 900 avg train loss = 5.5503\n",
            "Step 1000 avg train loss = 5.5670\n",
            "Step 1100 avg train loss = 5.5417\n",
            "Step 1200 avg train loss = 5.5513\n",
            "Validation loss after 9 epoch = 5.5379\n",
            "Step 0 avg train loss = 5.5147\n",
            "Step 100 avg train loss = 5.5248\n",
            "Step 200 avg train loss = 5.5213\n",
            "Step 300 avg train loss = 5.5253\n",
            "Step 400 avg train loss = 5.5136\n",
            "Step 500 avg train loss = 5.5128\n",
            "Step 600 avg train loss = 5.5002\n",
            "Step 700 avg train loss = 5.4977\n",
            "Step 800 avg train loss = 5.5050\n",
            "Step 900 avg train loss = 5.4913\n",
            "Step 1000 avg train loss = 5.4803\n",
            "Step 1100 avg train loss = 5.4991\n",
            "Step 1200 avg train loss = 5.4807\n",
            "Validation loss after 10 epoch = 5.5050\n",
            "Step 0 avg train loss = 5.3762\n",
            "Step 100 avg train loss = 5.4592\n",
            "Step 200 avg train loss = 5.4525\n",
            "Step 300 avg train loss = 5.4554\n",
            "Step 400 avg train loss = 5.4391\n",
            "Step 500 avg train loss = 5.4540\n",
            "Step 600 avg train loss = 5.4397\n",
            "Step 700 avg train loss = 5.4464\n",
            "Step 800 avg train loss = 5.4614\n",
            "Step 900 avg train loss = 5.4496\n",
            "Step 1000 avg train loss = 5.4385\n",
            "Step 1100 avg train loss = 5.4496\n",
            "Step 1200 avg train loss = 5.4286\n",
            "Validation loss after 11 epoch = 5.4796\n",
            "Step 0 avg train loss = 5.3345\n",
            "Step 100 avg train loss = 5.4158\n",
            "Step 200 avg train loss = 5.3989\n",
            "Step 300 avg train loss = 5.3882\n",
            "Step 400 avg train loss = 5.4133\n",
            "Step 500 avg train loss = 5.4101\n",
            "Step 600 avg train loss = 5.3996\n",
            "Step 700 avg train loss = 5.3847\n",
            "Step 800 avg train loss = 5.3985\n",
            "Step 900 avg train loss = 5.4010\n",
            "Step 1000 avg train loss = 5.3869\n",
            "Step 1100 avg train loss = 5.3773\n",
            "Step 1200 avg train loss = 5.3844\n",
            "Validation loss after 12 epoch = 5.4515\n",
            "Step 0 avg train loss = 5.3657\n",
            "Step 100 avg train loss = 5.3567\n",
            "Step 200 avg train loss = 5.3628\n",
            "Step 300 avg train loss = 5.3376\n",
            "Step 400 avg train loss = 5.3497\n",
            "Step 500 avg train loss = 5.3527\n",
            "Step 600 avg train loss = 5.3321\n",
            "Step 700 avg train loss = 5.3514\n",
            "Step 800 avg train loss = 5.3540\n",
            "Step 900 avg train loss = 5.3453\n",
            "Step 1000 avg train loss = 5.3384\n",
            "Step 1100 avg train loss = 5.3457\n",
            "Step 1200 avg train loss = 5.3508\n",
            "Validation loss after 13 epoch = 5.4317\n",
            "Step 0 avg train loss = 5.3898\n",
            "Step 100 avg train loss = 5.3117\n",
            "Step 200 avg train loss = 5.3240\n",
            "Step 300 avg train loss = 5.2970\n",
            "Step 400 avg train loss = 5.3147\n",
            "Step 500 avg train loss = 5.3066\n",
            "Step 600 avg train loss = 5.3018\n",
            "Step 700 avg train loss = 5.3155\n",
            "Step 800 avg train loss = 5.2939\n",
            "Step 900 avg train loss = 5.3080\n",
            "Step 1000 avg train loss = 5.2924\n",
            "Step 1100 avg train loss = 5.2907\n",
            "Step 1200 avg train loss = 5.2900\n",
            "Validation loss after 14 epoch = 5.4141\n",
            "Step 0 avg train loss = 5.3808\n",
            "Step 100 avg train loss = 5.2629\n",
            "Step 200 avg train loss = 5.2723\n",
            "Step 300 avg train loss = 5.2729\n",
            "Step 400 avg train loss = 5.2615\n",
            "Step 500 avg train loss = 5.2789\n",
            "Step 600 avg train loss = 5.2517\n",
            "Step 700 avg train loss = 5.2426\n",
            "Step 800 avg train loss = 5.2675\n",
            "Step 900 avg train loss = 5.2424\n",
            "Step 1000 avg train loss = 5.2651\n",
            "Step 1100 avg train loss = 5.2825\n",
            "Step 1200 avg train loss = 5.2472\n",
            "Validation loss after 15 epoch = 5.3946\n",
            "Step 0 avg train loss = 5.0620\n",
            "Step 100 avg train loss = 5.2311\n",
            "Step 200 avg train loss = 5.2257\n",
            "Step 300 avg train loss = 5.2289\n",
            "Step 400 avg train loss = 5.2266\n",
            "Step 500 avg train loss = 5.2268\n",
            "Step 600 avg train loss = 5.2172\n",
            "Step 700 avg train loss = 5.2259\n",
            "Step 800 avg train loss = 5.2242\n",
            "Step 900 avg train loss = 5.2381\n",
            "Step 1000 avg train loss = 5.2233\n",
            "Step 1100 avg train loss = 5.1969\n",
            "Step 1200 avg train loss = 5.2176\n",
            "Validation loss after 16 epoch = 5.3782\n",
            "Step 0 avg train loss = 5.1128\n",
            "Step 100 avg train loss = 5.1817\n",
            "Step 200 avg train loss = 5.1766\n",
            "Step 300 avg train loss = 5.1901\n",
            "Step 400 avg train loss = 5.1940\n",
            "Step 500 avg train loss = 5.1907\n",
            "Step 600 avg train loss = 5.1838\n",
            "Step 700 avg train loss = 5.1870\n",
            "Step 800 avg train loss = 5.1921\n",
            "Step 900 avg train loss = 5.1845\n",
            "Step 1000 avg train loss = 5.1878\n",
            "Step 1100 avg train loss = 5.1758\n",
            "Step 1200 avg train loss = 5.1895\n",
            "Validation loss after 17 epoch = 5.3643\n",
            "Step 0 avg train loss = 5.2440\n",
            "Step 100 avg train loss = 5.1561\n",
            "Step 200 avg train loss = 5.1449\n",
            "Step 300 avg train loss = 5.1452\n",
            "Step 400 avg train loss = 5.1676\n",
            "Step 500 avg train loss = 5.1518\n",
            "Step 600 avg train loss = 5.1414\n",
            "Step 700 avg train loss = 5.1534\n",
            "Step 800 avg train loss = 5.1318\n",
            "Step 900 avg train loss = 5.1535\n",
            "Step 1000 avg train loss = 5.1571\n",
            "Step 1100 avg train loss = 5.1553\n",
            "Step 1200 avg train loss = 5.1417\n",
            "Validation loss after 18 epoch = 5.3536\n",
            "Step 0 avg train loss = 5.1869\n",
            "Step 100 avg train loss = 5.1085\n",
            "Step 200 avg train loss = 5.1131\n",
            "Step 300 avg train loss = 5.1170\n",
            "Step 400 avg train loss = 5.1163\n",
            "Step 500 avg train loss = 5.1276\n",
            "Step 600 avg train loss = 5.1110\n",
            "Step 700 avg train loss = 5.1193\n",
            "Step 800 avg train loss = 5.1229\n",
            "Step 900 avg train loss = 5.1152\n",
            "Step 1000 avg train loss = 5.1231\n",
            "Step 1100 avg train loss = 5.1118\n",
            "Step 1200 avg train loss = 5.1135\n",
            "Validation loss after 19 epoch = 5.3481\n",
            "Step 0 avg train loss = 5.2126\n",
            "Step 100 avg train loss = 5.1007\n",
            "Step 200 avg train loss = 5.0622\n",
            "Step 300 avg train loss = 5.0669\n",
            "Step 400 avg train loss = 5.0817\n",
            "Step 500 avg train loss = 5.0839\n",
            "Step 600 avg train loss = 5.0894\n",
            "Step 700 avg train loss = 5.0965\n",
            "Step 800 avg train loss = 5.0852\n",
            "Step 900 avg train loss = 5.0829\n",
            "Step 1000 avg train loss = 5.0830\n",
            "Step 1100 avg train loss = 5.1004\n",
            "Step 1200 avg train loss = 5.0872\n",
            "Validation loss after 20 epoch = 5.3367\n",
            "Step 0 avg train loss = 5.0036\n",
            "Step 100 avg train loss = 5.0570\n",
            "Step 200 avg train loss = 5.0414\n",
            "Step 300 avg train loss = 5.0337\n",
            "Step 400 avg train loss = 5.0347\n",
            "Step 500 avg train loss = 5.0576\n",
            "Step 600 avg train loss = 5.0680\n",
            "Step 700 avg train loss = 5.0524\n",
            "Step 800 avg train loss = 5.0474\n",
            "Step 900 avg train loss = 5.0696\n",
            "Step 1000 avg train loss = 5.0470\n",
            "Step 1100 avg train loss = 5.0635\n",
            "Step 1200 avg train loss = 5.0664\n",
            "Validation loss after 21 epoch = 5.3290\n",
            "Step 0 avg train loss = 4.9635\n",
            "Step 100 avg train loss = 5.0199\n",
            "Step 200 avg train loss = 5.0047\n",
            "Step 300 avg train loss = 5.0239\n",
            "Step 400 avg train loss = 5.0207\n",
            "Step 500 avg train loss = 5.0171\n",
            "Step 600 avg train loss = 5.0262\n",
            "Step 700 avg train loss = 5.0161\n",
            "Step 800 avg train loss = 5.0261\n",
            "Step 900 avg train loss = 5.0383\n",
            "Step 1000 avg train loss = 5.0346\n",
            "Step 1100 avg train loss = 5.0178\n",
            "Step 1200 avg train loss = 5.0394\n",
            "Validation loss after 22 epoch = 5.3240\n",
            "Step 0 avg train loss = 5.1289\n",
            "Step 100 avg train loss = 4.9938\n",
            "Step 200 avg train loss = 5.0032\n",
            "Step 300 avg train loss = 5.0077\n",
            "Step 400 avg train loss = 5.0032\n",
            "Step 500 avg train loss = 5.0000\n",
            "Step 600 avg train loss = 4.9881\n",
            "Step 700 avg train loss = 4.9932\n",
            "Step 800 avg train loss = 5.0005\n",
            "Step 900 avg train loss = 4.9972\n",
            "Step 1000 avg train loss = 4.9759\n",
            "Step 1100 avg train loss = 5.0014\n",
            "Step 1200 avg train loss = 4.9858\n",
            "Validation loss after 23 epoch = 5.3165\n",
            "Step 0 avg train loss = 4.9337\n",
            "Step 100 avg train loss = 4.9566\n",
            "Step 200 avg train loss = 4.9712\n",
            "Step 300 avg train loss = 4.9727\n",
            "Step 400 avg train loss = 4.9611\n",
            "Step 500 avg train loss = 4.9712\n",
            "Step 600 avg train loss = 4.9442\n",
            "Step 700 avg train loss = 4.9670\n",
            "Step 800 avg train loss = 4.9759\n",
            "Step 900 avg train loss = 4.9599\n",
            "Step 1000 avg train loss = 4.9838\n",
            "Step 1100 avg train loss = 4.9726\n",
            "Step 1200 avg train loss = 4.9842\n",
            "Validation loss after 24 epoch = 5.3124\n",
            "Step 0 avg train loss = 5.0206\n",
            "Step 100 avg train loss = 4.9393\n",
            "Step 200 avg train loss = 4.9491\n",
            "Step 300 avg train loss = 4.9356\n",
            "Step 400 avg train loss = 4.9468\n",
            "Step 500 avg train loss = 4.9377\n",
            "Step 600 avg train loss = 4.9414\n",
            "Step 700 avg train loss = 4.9485\n",
            "Step 800 avg train loss = 4.9414\n",
            "Step 900 avg train loss = 4.9465\n",
            "Step 1000 avg train loss = 4.9349\n",
            "Step 1100 avg train loss = 4.9350\n",
            "Step 1200 avg train loss = 4.9524\n",
            "Validation loss after 25 epoch = 5.3051\n",
            "Step 0 avg train loss = 4.9117\n",
            "Step 100 avg train loss = 4.9192\n",
            "Step 200 avg train loss = 4.9233\n",
            "Step 300 avg train loss = 4.8944\n",
            "Step 400 avg train loss = 4.9205\n",
            "Step 500 avg train loss = 4.9148\n",
            "Step 600 avg train loss = 4.9218\n",
            "Step 700 avg train loss = 4.9214\n",
            "Step 800 avg train loss = 4.9108\n",
            "Step 900 avg train loss = 4.9196\n",
            "Step 1000 avg train loss = 4.9171\n",
            "Step 1100 avg train loss = 4.9187\n",
            "Step 1200 avg train loss = 4.9134\n",
            "Validation loss after 26 epoch = 5.3009\n",
            "Step 0 avg train loss = 4.7447\n",
            "Step 100 avg train loss = 4.8698\n",
            "Step 200 avg train loss = 4.8660\n",
            "Step 300 avg train loss = 4.8847\n",
            "Step 400 avg train loss = 4.8735\n",
            "Step 500 avg train loss = 4.8956\n",
            "Step 600 avg train loss = 4.9066\n",
            "Step 700 avg train loss = 4.8934\n",
            "Step 800 avg train loss = 4.8962\n",
            "Step 900 avg train loss = 4.9080\n",
            "Step 1000 avg train loss = 4.8991\n",
            "Step 1100 avg train loss = 4.8934\n",
            "Step 1200 avg train loss = 4.9082\n",
            "Validation loss after 27 epoch = 5.3032\n",
            "Step 0 avg train loss = 4.8745\n",
            "Step 100 avg train loss = 4.8647\n",
            "Step 200 avg train loss = 4.8537\n",
            "Step 300 avg train loss = 4.8629\n",
            "Step 400 avg train loss = 4.8798\n",
            "Step 500 avg train loss = 4.8629\n",
            "Step 600 avg train loss = 4.8685\n",
            "Step 700 avg train loss = 4.8567\n",
            "Step 800 avg train loss = 4.8610\n",
            "Step 900 avg train loss = 4.8721\n",
            "Step 1000 avg train loss = 4.8804\n",
            "Step 1100 avg train loss = 4.8678\n",
            "Step 1200 avg train loss = 4.8798\n",
            "Validation loss after 28 epoch = 5.2965\n",
            "Step 0 avg train loss = 4.7886\n",
            "Step 100 avg train loss = 4.8404\n",
            "Step 200 avg train loss = 4.8354\n",
            "Step 300 avg train loss = 4.8283\n",
            "Step 400 avg train loss = 4.8324\n",
            "Step 500 avg train loss = 4.8381\n",
            "Step 600 avg train loss = 4.8484\n",
            "Step 700 avg train loss = 4.8404\n",
            "Step 800 avg train loss = 4.8317\n",
            "Step 900 avg train loss = 4.8547\n",
            "Step 1000 avg train loss = 4.8436\n",
            "Step 1100 avg train loss = 4.8564\n",
            "Step 1200 avg train loss = 4.8722\n",
            "Validation loss after 29 epoch = 5.2958\n",
            "Step 0 avg train loss = 4.7696\n",
            "Step 100 avg train loss = 4.8142\n",
            "Step 200 avg train loss = 4.8247\n",
            "Step 300 avg train loss = 4.8153\n",
            "Step 400 avg train loss = 4.8158\n",
            "Step 500 avg train loss = 4.8159\n",
            "Step 600 avg train loss = 4.8218\n",
            "Step 700 avg train loss = 4.8161\n",
            "Step 800 avg train loss = 4.8145\n",
            "Step 900 avg train loss = 4.8240\n",
            "Step 1000 avg train loss = 4.8306\n",
            "Step 1100 avg train loss = 4.8225\n",
            "Step 1200 avg train loss = 4.8301\n",
            "Validation loss after 30 epoch = 5.2910\n",
            "Step 0 avg train loss = 4.8889\n",
            "Step 100 avg train loss = 4.7881\n",
            "Step 200 avg train loss = 4.7923\n",
            "Step 300 avg train loss = 4.7973\n",
            "Step 400 avg train loss = 4.7807\n",
            "Step 500 avg train loss = 4.7832\n",
            "Step 600 avg train loss = 4.8083\n",
            "Step 700 avg train loss = 4.7983\n",
            "Step 800 avg train loss = 4.8004\n",
            "Step 900 avg train loss = 4.8036\n",
            "Step 1000 avg train loss = 4.8134\n",
            "Step 1100 avg train loss = 4.7931\n",
            "Step 1200 avg train loss = 4.8018\n",
            "Validation loss after 31 epoch = 5.2909\n",
            "Step 0 avg train loss = 4.8530\n",
            "Step 100 avg train loss = 4.7734\n",
            "Step 200 avg train loss = 4.7545\n",
            "Step 300 avg train loss = 4.7644\n",
            "Step 400 avg train loss = 4.7644\n",
            "Step 500 avg train loss = 4.7698\n",
            "Step 600 avg train loss = 4.7744\n",
            "Step 700 avg train loss = 4.7863\n",
            "Step 800 avg train loss = 4.7766\n",
            "Step 900 avg train loss = 4.7819\n",
            "Step 1000 avg train loss = 4.7900\n",
            "Step 1100 avg train loss = 4.7879\n",
            "Step 1200 avg train loss = 4.7824\n",
            "Validation loss after 32 epoch = 5.2879\n",
            "Step 0 avg train loss = 4.7934\n",
            "Step 100 avg train loss = 4.7523\n",
            "Step 200 avg train loss = 4.7553\n",
            "Step 300 avg train loss = 4.7645\n",
            "Step 400 avg train loss = 4.7294\n",
            "Step 500 avg train loss = 4.7428\n",
            "Step 600 avg train loss = 4.7527\n",
            "Step 700 avg train loss = 4.7545\n",
            "Step 800 avg train loss = 4.7557\n",
            "Step 900 avg train loss = 4.7511\n",
            "Step 1000 avg train loss = 4.7492\n",
            "Step 1100 avg train loss = 4.7653\n",
            "Step 1200 avg train loss = 4.7687\n",
            "Validation loss after 33 epoch = 5.2880\n",
            "Step 0 avg train loss = 4.7763\n",
            "Step 100 avg train loss = 4.7191\n",
            "Step 200 avg train loss = 4.7262\n",
            "Step 300 avg train loss = 4.7252\n",
            "Step 400 avg train loss = 4.7289\n",
            "Step 500 avg train loss = 4.7321\n",
            "Step 600 avg train loss = 4.7219\n",
            "Step 700 avg train loss = 4.7422\n",
            "Step 800 avg train loss = 4.7334\n",
            "Step 900 avg train loss = 4.7335\n",
            "Step 1000 avg train loss = 4.7396\n",
            "Step 1100 avg train loss = 4.7459\n",
            "Step 1200 avg train loss = 4.7366\n",
            "Validation loss after 34 epoch = 5.2910\n",
            "Step 0 avg train loss = 4.6951\n",
            "Step 100 avg train loss = 4.6868\n",
            "Step 200 avg train loss = 4.7165\n",
            "Step 300 avg train loss = 4.7050\n",
            "Step 400 avg train loss = 4.7159\n",
            "Step 500 avg train loss = 4.7107\n",
            "Step 600 avg train loss = 4.7016\n",
            "Step 700 avg train loss = 4.7048\n",
            "Step 800 avg train loss = 4.6974\n",
            "Step 900 avg train loss = 4.7063\n",
            "Step 1000 avg train loss = 4.7298\n",
            "Step 1100 avg train loss = 4.7413\n",
            "Step 1200 avg train loss = 4.7193\n",
            "Validation loss after 35 epoch = 5.2888\n",
            "Step 0 avg train loss = 4.7224\n",
            "Step 100 avg train loss = 4.6756\n",
            "Step 200 avg train loss = 4.6875\n",
            "Step 300 avg train loss = 4.6848\n",
            "Step 400 avg train loss = 4.6615\n",
            "Step 500 avg train loss = 4.6861\n",
            "Step 600 avg train loss = 4.6932\n",
            "Step 700 avg train loss = 4.7044\n",
            "Step 800 avg train loss = 4.6874\n",
            "Step 900 avg train loss = 4.6986\n",
            "Step 1000 avg train loss = 4.7002\n",
            "Step 1100 avg train loss = 4.7091\n",
            "Step 1200 avg train loss = 4.7182\n",
            "Validation loss after 36 epoch = 5.2944\n",
            "Step 0 avg train loss = 4.5434\n",
            "Step 100 avg train loss = 4.6413\n",
            "Step 200 avg train loss = 4.6671\n",
            "Step 300 avg train loss = 4.6682\n",
            "Step 400 avg train loss = 4.6592\n",
            "Step 500 avg train loss = 4.6766\n",
            "Step 600 avg train loss = 4.6749\n",
            "Step 700 avg train loss = 4.6573\n",
            "Step 800 avg train loss = 4.6775\n",
            "Step 900 avg train loss = 4.6788\n",
            "Step 1000 avg train loss = 4.6891\n",
            "Step 1100 avg train loss = 4.6913\n",
            "Step 1200 avg train loss = 4.6882\n",
            "Validation loss after 37 epoch = 5.2902\n",
            "Step 0 avg train loss = 4.7061\n",
            "Step 100 avg train loss = 4.6518\n",
            "Step 200 avg train loss = 4.6461\n",
            "Step 300 avg train loss = 4.6316\n",
            "Step 400 avg train loss = 4.6387\n",
            "Step 500 avg train loss = 4.6386\n",
            "Step 600 avg train loss = 4.6560\n",
            "Step 700 avg train loss = 4.6769\n",
            "Step 800 avg train loss = 4.6588\n",
            "Step 900 avg train loss = 4.6519\n",
            "Step 1000 avg train loss = 4.6696\n",
            "Step 1100 avg train loss = 4.6664\n",
            "Step 1200 avg train loss = 4.6668\n",
            "Validation loss after 38 epoch = 5.2939\n",
            "Step 0 avg train loss = 4.6333\n",
            "Step 100 avg train loss = 4.6077\n",
            "Step 200 avg train loss = 4.6224\n",
            "Step 300 avg train loss = 4.6217\n",
            "Step 400 avg train loss = 4.6370\n",
            "Step 500 avg train loss = 4.6300\n",
            "Step 600 avg train loss = 4.6309\n",
            "Step 700 avg train loss = 4.6564\n",
            "Step 800 avg train loss = 4.6335\n",
            "Step 900 avg train loss = 4.6414\n",
            "Step 1000 avg train loss = 4.6263\n",
            "Step 1100 avg train loss = 4.6608\n",
            "Step 1200 avg train loss = 4.6453\n",
            "Validation loss after 39 epoch = 5.2957\n",
            "Step 0 avg train loss = 4.5276\n",
            "Step 100 avg train loss = 4.6237\n",
            "Step 200 avg train loss = 4.6098\n",
            "Step 300 avg train loss = 4.6082\n",
            "Step 400 avg train loss = 4.6013\n",
            "Step 500 avg train loss = 4.6094\n",
            "Step 600 avg train loss = 4.6184\n",
            "Step 700 avg train loss = 4.6091\n",
            "Step 800 avg train loss = 4.6165\n",
            "Step 900 avg train loss = 4.6063\n",
            "Step 1000 avg train loss = 4.6298\n",
            "Step 1100 avg train loss = 4.6210\n",
            "Step 1200 avg train loss = 4.6356\n",
            "Validation loss after 40 epoch = 5.2964\n",
            "Step 0 avg train loss = 4.6999\n",
            "Step 100 avg train loss = 4.5700\n",
            "Step 200 avg train loss = 4.5880\n",
            "Step 300 avg train loss = 4.5833\n",
            "Step 400 avg train loss = 4.6025\n",
            "Step 500 avg train loss = 4.6020\n",
            "Step 600 avg train loss = 4.5906\n",
            "Step 700 avg train loss = 4.5948\n",
            "Step 800 avg train loss = 4.5994\n",
            "Step 900 avg train loss = 4.6049\n",
            "Step 1000 avg train loss = 4.6091\n",
            "Step 1100 avg train loss = 4.6107\n",
            "Step 1200 avg train loss = 4.6160\n",
            "Validation loss after 41 epoch = 5.2962\n",
            "Step 0 avg train loss = 4.6247\n",
            "Step 100 avg train loss = 4.5718\n",
            "Step 200 avg train loss = 4.5619\n",
            "Step 300 avg train loss = 4.5608\n",
            "Step 400 avg train loss = 4.5828\n",
            "Step 500 avg train loss = 4.5577\n",
            "Step 600 avg train loss = 4.5777\n",
            "Step 700 avg train loss = 4.5780\n",
            "Step 800 avg train loss = 4.6010\n",
            "Step 900 avg train loss = 4.5830\n",
            "Step 1000 avg train loss = 4.5720\n",
            "Step 1100 avg train loss = 4.6026\n",
            "Step 1200 avg train loss = 4.6013\n",
            "Validation loss after 42 epoch = 5.3026\n",
            "Step 0 avg train loss = 4.6130\n",
            "Step 100 avg train loss = 4.5390\n",
            "Step 200 avg train loss = 4.5630\n",
            "Step 300 avg train loss = 4.5503\n",
            "Step 400 avg train loss = 4.5590\n",
            "Step 500 avg train loss = 4.5431\n",
            "Step 600 avg train loss = 4.5558\n",
            "Step 700 avg train loss = 4.5596\n",
            "Step 800 avg train loss = 4.5710\n",
            "Step 900 avg train loss = 4.5672\n",
            "Step 1000 avg train loss = 4.5830\n",
            "Step 1100 avg train loss = 4.5812\n",
            "Step 1200 avg train loss = 4.5627\n",
            "Validation loss after 43 epoch = 5.3037\n",
            "Step 0 avg train loss = 4.5496\n",
            "Step 100 avg train loss = 4.5393\n",
            "Step 200 avg train loss = 4.5285\n",
            "Step 300 avg train loss = 4.5364\n",
            "Step 400 avg train loss = 4.5164\n",
            "Step 500 avg train loss = 4.5581\n",
            "Step 600 avg train loss = 4.5432\n",
            "Step 700 avg train loss = 4.5410\n",
            "Step 800 avg train loss = 4.5507\n",
            "Step 900 avg train loss = 4.5412\n",
            "Step 1000 avg train loss = 4.5445\n",
            "Step 1100 avg train loss = 4.5602\n",
            "Step 1200 avg train loss = 4.5629\n",
            "Validation loss after 44 epoch = 5.3082\n",
            "Step 0 avg train loss = 4.5048\n",
            "Step 100 avg train loss = 4.5051\n",
            "Step 200 avg train loss = 4.4976\n",
            "Step 300 avg train loss = 4.5205\n",
            "Step 400 avg train loss = 4.5073\n",
            "Step 500 avg train loss = 4.5205\n",
            "Step 600 avg train loss = 4.5270\n",
            "Step 700 avg train loss = 4.5260\n",
            "Step 800 avg train loss = 4.5363\n",
            "Step 900 avg train loss = 4.5335\n",
            "Step 1000 avg train loss = 4.5371\n",
            "Step 1100 avg train loss = 4.5444\n",
            "Step 1200 avg train loss = 4.5563\n",
            "Validation loss after 45 epoch = 5.3105\n",
            "Step 0 avg train loss = 4.5361\n",
            "Step 100 avg train loss = 4.4877\n",
            "Step 200 avg train loss = 4.4862\n",
            "Step 300 avg train loss = 4.5081\n",
            "Step 400 avg train loss = 4.5182\n",
            "Step 500 avg train loss = 4.5007\n",
            "Step 600 avg train loss = 4.5063\n",
            "Step 700 avg train loss = 4.5307\n",
            "Step 800 avg train loss = 4.5180\n",
            "Step 900 avg train loss = 4.5083\n",
            "Step 1000 avg train loss = 4.5224\n",
            "Step 1100 avg train loss = 4.5285\n",
            "Step 1200 avg train loss = 4.5111\n",
            "Validation loss after 46 epoch = 5.3183\n",
            "Step 0 avg train loss = 4.4308\n",
            "Step 100 avg train loss = 4.4824\n",
            "Step 200 avg train loss = 4.4807\n",
            "Step 300 avg train loss = 4.4746\n",
            "Step 400 avg train loss = 4.4830\n",
            "Step 500 avg train loss = 4.4989\n",
            "Step 600 avg train loss = 4.4906\n",
            "Step 700 avg train loss = 4.4820\n",
            "Step 800 avg train loss = 4.4993\n",
            "Step 900 avg train loss = 4.5022\n",
            "Step 1000 avg train loss = 4.4978\n",
            "Step 1100 avg train loss = 4.5127\n",
            "Step 1200 avg train loss = 4.5161\n",
            "Validation loss after 47 epoch = 5.3178\n",
            "Step 0 avg train loss = 4.4908\n",
            "Step 100 avg train loss = 4.4553\n",
            "Step 200 avg train loss = 4.4608\n",
            "Step 300 avg train loss = 4.4611\n",
            "Step 400 avg train loss = 4.4760\n",
            "Step 500 avg train loss = 4.4620\n",
            "Step 600 avg train loss = 4.4883\n",
            "Step 700 avg train loss = 4.4698\n",
            "Step 800 avg train loss = 4.4890\n",
            "Step 900 avg train loss = 4.4796\n",
            "Step 1000 avg train loss = 4.5011\n",
            "Step 1100 avg train loss = 4.4956\n",
            "Step 1200 avg train loss = 4.4777\n",
            "Validation loss after 48 epoch = 5.3211\n",
            "Step 0 avg train loss = 4.5534\n",
            "Step 100 avg train loss = 4.4434\n",
            "Step 200 avg train loss = 4.4415\n",
            "Step 300 avg train loss = 4.4533\n",
            "Step 400 avg train loss = 4.4713\n",
            "Step 500 avg train loss = 4.4714\n",
            "Step 600 avg train loss = 4.4459\n",
            "Step 700 avg train loss = 4.4558\n",
            "Step 800 avg train loss = 4.4631\n",
            "Step 900 avg train loss = 4.4757\n",
            "Step 1000 avg train loss = 4.4701\n",
            "Step 1100 avg train loss = 4.4671\n",
            "Step 1200 avg train loss = 4.4760\n",
            "Validation loss after 49 epoch = 5.3206\n",
            "Step 0 avg train loss = 4.5120\n",
            "Step 100 avg train loss = 4.4394\n",
            "Step 200 avg train loss = 4.4369\n",
            "Step 300 avg train loss = 4.4312\n",
            "Step 400 avg train loss = 4.4313\n",
            "Step 500 avg train loss = 4.4288\n",
            "Step 600 avg train loss = 4.4378\n",
            "Step 700 avg train loss = 4.4541\n",
            "Step 800 avg train loss = 4.4463\n",
            "Step 900 avg train loss = 4.4537\n",
            "Step 1000 avg train loss = 4.4663\n",
            "Step 1100 avg train loss = 4.4607\n",
            "Step 1200 avg train loss = 4.4566\n",
            "Validation loss after 50 epoch = 5.3295\n",
            "Step 0 avg train loss = 4.3465\n",
            "Step 100 avg train loss = 4.4100\n",
            "Step 200 avg train loss = 4.4132\n",
            "Step 300 avg train loss = 4.4207\n",
            "Step 400 avg train loss = 4.4216\n",
            "Step 500 avg train loss = 4.4263\n",
            "Step 600 avg train loss = 4.4291\n",
            "Step 700 avg train loss = 4.4163\n",
            "Step 800 avg train loss = 4.4415\n",
            "Step 900 avg train loss = 4.4353\n",
            "Step 1000 avg train loss = 4.4316\n",
            "Step 1100 avg train loss = 4.4492\n",
            "Step 1200 avg train loss = 4.4436\n",
            "Validation loss after 51 epoch = 5.3370\n",
            "Step 0 avg train loss = 4.5066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o36n96Zlr6bD",
        "colab_type": "text"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7sb5G4nxZCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  plot_cache = torch.load(os.path.join(path_root,'plot_acc'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrxjUQ5ar6bE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "epochs = numpy.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [i[0] for i in plot_cache], label='Train loss')\n",
        "plt.plot(epochs, [i[1] for i in plot_cache], label='Valid loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Loss curves')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNrSxzfr_3Ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "epochs = numpy.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in plot_cache], label='Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/numpy.log(2)) for i in plot_cache], label='Valid ppl')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('PPL curves')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezcT8LnBr6bG",
        "colab_type": "text"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPU7boWTr6bH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhBBjOT-r6bP",
        "colab_type": "text"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLKqaLMnr6bR",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHc0H9C-r6bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pylab inline \n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w6VUE1Cr6bU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Vsize = 100                                 # e.g. len(dictionary)\n",
        "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
        "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
        "\n",
        "words = ['the', 'dog', 'ran']\n",
        "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(fake_weight_matrix, word_ids, words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fso4gxawr6bW",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAsCySp6r6bX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzrZFM5ur6bc",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1Rbp3Pdr6bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpt0CoKtr6bg",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDm3mkdGr6bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-tAXoAGr6bk",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVUIn3Wbr6bl",
        "colab_type": "text"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbO_WoAVr6bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MnbGf9ur6bt",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.2 Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT5fm9LTr6bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCpNetODr6bw",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3YmQwDlr6bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLMSeeAgr6b0",
        "colab_type": "text"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFkw8QF2r6b1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBrslckfr6b3",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFc3Yqxpr6b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VaZwUUfr6b6",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUotf6Lpr6b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}